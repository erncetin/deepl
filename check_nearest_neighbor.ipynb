{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, DDIMScheduler, DDPMScheduler\n",
    "import torch\n",
    "from config_file import TrainingConfig\n",
    "from CustomDataset import CustomDateset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nearest_neighbors(gen_images, real_images, num_to_check, metric='l2', device='cuda', batch_size=1000):\n",
    "    \"\"\"\n",
    "    Finds and visualizes the nearest neighbor in the real dataset for a set of generated images.\n",
    "    \n",
    "    Args:\n",
    "        gen_images (torch.Tensor): Generated images (B, C, H, W), values in [0, 1]\n",
    "        real_images (torch.Tensor): Real dataset images (N, C, H, W), values in [0, 1]\n",
    "        num_to_check (int): Number of random images to visualize\n",
    "        metric (str): 'l1' (Manhattan) or 'l2' (Euclidean) distance\n",
    "        device (str): Device for calculation ('cuda' or 'cpu')\n",
    "        batch_size (int): Size of chunks for processing real images (saves memory)\n",
    "    \"\"\"\n",
    "    print(f\"Computing nearest neighbors using {metric.upper()} distance...\")\n",
    "    \n",
    "    # 1. Randomly select images to check (instead of just the first N)\n",
    "    n_gen = gen_images.shape[0]\n",
    "    if n_gen > num_to_check:\n",
    "        indices = torch.randperm(n_gen)[:num_to_check]\n",
    "        query_images = gen_images[indices].to(device)\n",
    "    else:\n",
    "        query_images = gen_images.to(device)\n",
    "        num_to_check = n_gen\n",
    "\n",
    "    # Flatten query images: (num_to_check, features)\n",
    "    query_flat = query_images.reshape(query_images.shape[0], -1)\n",
    "    \n",
    "    # Storage for results\n",
    "    min_dists = torch.full((num_to_check,), float('inf'), device=device)\n",
    "    min_indices = torch.zeros((num_to_check,), dtype=torch.long, device=device)\n",
    "    \n",
    "    # 2. Process real images in batches to avoid OOM on GPU\n",
    "    n_real = real_images.shape[0]\n",
    "    \n",
    "    # We loop over the real dataset\n",
    "    for i in range(0, n_real, batch_size):\n",
    "        end = min(i + batch_size, n_real)\n",
    "        real_batch = real_images[i:end].to(device)\n",
    "        \n",
    "        # Flatten real batch: (batch_size, features)\n",
    "        real_flat = real_batch.reshape(real_batch.shape[0], -1)\n",
    "        \n",
    "        # Compute distances\n",
    "        # shapes: query_flat (M, D), real_flat (Batch, D)\n",
    "        if metric == 'l1':\n",
    "            # L1 = sum(|x - y|) -> efficient via cdist with p=1\n",
    "            dists = torch.cdist(query_flat, real_flat, p=1)\n",
    "        else:\n",
    "            # L2 = sqrt(sum((x - y)^2))\n",
    "            dists = torch.cdist(query_flat, real_flat, p=2)\n",
    "            \n",
    "        # Find min for this batch\n",
    "        batch_min_dists, batch_min_idx = dists.min(dim=1)\n",
    "        \n",
    "        # Update global minimums\n",
    "        mask = batch_min_dists < min_dists\n",
    "        min_dists[mask] = batch_min_dists[mask]\n",
    "        min_indices[mask] = batch_min_idx[mask] + i # Add offset 'i'\n",
    "        \n",
    "    # 3. Plotting\n",
    "    print(\"Plotting results...\")\n",
    "    # Move results to CPU for plotting\n",
    "    query_images = query_images.cpu()\n",
    "    real_images_subset = real_images[min_indices.cpu()].cpu() # Fetch only the winners\n",
    "    min_dists = min_dists.cpu()\n",
    "\n",
    "    # Create figure: 3 columns (Gen, Nearest Real, Diff)\n",
    "    fig, axes = plt.subplots(num_to_check, 3, figsize=(12, 3 * num_to_check))\n",
    "    \n",
    "    if num_to_check == 1: axes = [axes] # Handle single row case\n",
    "\n",
    "    for i in range(num_to_check):\n",
    "        gen_np = query_images[i].permute(1, 2, 0).numpy()\n",
    "        real_np = real_images_subset[i].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Calculate Difference Map (Absolute difference)\n",
    "        diff_map = np.abs(gen_np - real_np)\n",
    "        diff_map = diff_map.mean(axis=2) # Average over channels for grayscale heatmap\n",
    "\n",
    "        # Column 1: Generated\n",
    "        axes[i][0].imshow(np.clip(gen_np, 0, 1))\n",
    "        axes[i][0].set_title(\"Generated\")\n",
    "        axes[i][0].axis(\"off\")\n",
    "\n",
    "        # Column 2: Nearest Real\n",
    "        axes[i][1].imshow(np.clip(real_np, 0, 1))\n",
    "        axes[i][1].set_title(f\"Nearest Real\\n({metric.upper()}: {min_dists[i]:.2f})\")\n",
    "        axes[i][1].axis(\"off\")\n",
    "        \n",
    "        # Column 3: Difference Map\n",
    "        im = axes[i][2].imshow(diff_map, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[i][2].set_title(\"Difference Map\")\n",
    "        axes[i][2].axis(\"off\")\n",
    "        \n",
    "        # Optional: Add borders to diff map to make it distinct\n",
    "        for spine in axes[i][2].spines.values():\n",
    "            spine.set_edgecolor('red')\n",
    "            spine.set_linewidth(1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"nearest_neighbors_check1.png\")\n",
    "    plt.show()\n",
    "    print(\"Saved plot to nearest_neighbors_check.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "path = \"C:/Users/HP/.cache/kagglehub/datasets/ebrahimelgazar/pixel-art/versions/1\"\n",
    "#force use local model\n",
    "model = UNet2DConditionModel.from_pretrained(\"C:/Users/HP/Desktop/VSCODE_FILES/deep_learning_project/pixelart_ddpm_version4/unet\", use_safetensors=True)\n",
    "model.to(device)\n",
    "dataset = CustomDateset(path + \"/sprites.npy\", path + \"/sprites_labels.npy\")\n",
    "dataloader = DataLoader(dataset, batch_size = config.train_batch_size, shuffle = True, num_workers=2, pin_memory=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_real_images = []\n",
    "with torch.no_grad():\n",
    "    for real_images, _ in tqdm(dataloader, desc=\"FID real\"):\n",
    "        real_images = real_images.to(device)\n",
    "        real_images = ((real_images + 1) / 2).clamp(0, 1)\n",
    "        all_real_images.append(real_images.cpu())\n",
    "all_real_images = torch.cat(all_real_images, dim = 0)  # (N, C, H, W)\n",
    "\n",
    "target_class = 3\n",
    "real_class_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in dataloader:\n",
    "        mask = labels == target_class\n",
    "        if mask.any():\n",
    "            imgs = imgs[mask]\n",
    "            imgs = ((imgs + 1) / 2).clamp(0, 1)\n",
    "            real_class_images.append(imgs)\n",
    "\n",
    "real_class_images = torch.cat(real_class_images, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. class =  hayvanlar = zor öğreniyo\n",
    "\n",
    "# 3. class = eşyalar,silahlar = kolay öğrendikleri\n",
    "# 4. class = silahlı npcler = orta zorluk?\n",
    "guidance_scale = 5.0\n",
    "null_class = dataset.labels.max().item() + 1\n",
    "# real images\n",
    "\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise_scheduler = DDIMScheduler.from_config(noise_scheduler.config)\n",
    "noise_scheduler.set_timesteps(25)\n",
    "\n",
    "    # fake images\n",
    "all_fake_images = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        cond_label = torch.tensor([target_class], device=device, dtype=torch.long)\n",
    "        uncond_label = torch.tensor([null_class], device=device, dtype=torch.long)\n",
    "\n",
    "        combined_labels = torch.cat([cond_label, uncond_label])\n",
    "        \n",
    "\n",
    "        fake_images = torch.randn((1,\n",
    "                                3,\n",
    "                                    config.image_size, \n",
    "                                    config.image_size), \n",
    "                                    device=device)\n",
    "            \n",
    "        for t in noise_scheduler.timesteps:\n",
    "            latent_model_input = torch.cat([fake_images] * 2)\n",
    "            latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "            noise_pred = model(latent_model_input, \n",
    "                                t, \n",
    "                                class_labels=combined_labels,\n",
    "                                return_dict=False,\n",
    "                                encoder_hidden_states = None)[0]\n",
    "            noise_pred_cond, noise_pred_uncond = noise_pred.chunk(2)\n",
    "\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "            fake_images = noise_scheduler.step(noise_pred,t,fake_images).prev_sample\n",
    "            \n",
    "        fake_images = ((fake_images.clamp(-1, 1) + 1) / 2).clamp(0, 1)\n",
    "        all_fake_images.append(fake_images.cpu())\n",
    "\n",
    "all_fake_images = torch.cat(all_fake_images, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.randn((1, 3, config.image_size, config.image_size), device=device)\n",
    "    t = torch.tensor([500], device=device)\n",
    "\n",
    "    out_a = model(x, t, class_labels=torch.tensor([0], device=device), encoder_hidden_states = None).sample\n",
    "    out_b = model(x, t, class_labels=torch.tensor([4], device=device), encoder_hidden_states = None).sample\n",
    "\n",
    "    print(torch.mean(torch.abs(out_a - out_b)).item())\n",
    "print(model.class_embedding)\n",
    "print(model.class_embedding.linear_1.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nearest_neighbors(all_fake_images, real_class_images, num_to_check=20, metric='l2', device = device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35000 images for Class 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load the raw data\n",
    "images = np.load(path + \"/sprites.npy\")\n",
    "raw_labels = np.load(path + \"/sprites_labels.npy\")\n",
    "\n",
    "# 2. Convert One-Hot to Indices (Just like your Dataset class)\n",
    "if raw_labels.ndim == 2:\n",
    "    labels = np.argmax(raw_labels, axis=1)\n",
    "else:\n",
    "    labels = raw_labels\n",
    "\n",
    "# 3. NOW search for the class (e.g., Class 3 for NPCs)\n",
    "target_check = 3 \n",
    "indices = np.where(labels == target_check)[0]\n",
    "\n",
    "print(f\"Found {len(indices)} images for Class {target_check}\")\n",
    "\n",
    "# 4. Visualize\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(min(5, len(indices))): # Safety check in case < 5 images\n",
    "    idx = indices[i]\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(images[idx]) \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Label {target_check}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
